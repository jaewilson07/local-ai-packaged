Metadata-Version: 2.4
Name: local-ai-packaged
Version: 0.1.0
Summary: Local AI packaged services with Supabase, n8n, Flowise, and more
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: requests>=2.31.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: pyyaml>=6.0
Dynamic: license-file

# Self-hosted AI Package

**Self-hosted AI Package** is an open, docker compose template that
quickly bootstraps a fully featured Local AI and Low Code development
environment including Ollama for your local LLMs, Open WebUI for an interface to chat with your N8N agents, and Supabase for your database, vector store, and authentication.

This is Cole's version with a couple of improvements and the addition of Supabase, Open WebUI, Flowise, Neo4j, Langfuse, SearXNG, and Caddy!
Also, the local RAG AI Agent workflows from the video will be automatically in your
n8n instance if you use this setup instead of the base one provided by n8n!

**IMPORTANT**: Supabase has updated a couple environment variables so you may have to add some new default values in your .env that I have in my .env_sample if you have had this project up and running already and are just pulling new changes. Specifically, you need to add "POOLER_DB_POOL_SIZE=5" to your .env. This is required if you have had the package running before June 14th.

## Important Links

- [Local AI community](https://thinktank.ottomator.ai/c/local-ai/18) forum over in the oTTomator Think Tank

- [GitHub Kanban board](https://github.com/users/coleam00/projects/2/views/1) for feature implementation and bug squashing.

- [Original Local AI Starter Kit](https://github.com/n8n-io/self-hosted-ai-starter-kit) by the n8n team

- Download my N8N + OpenWebUI integration [directly on the Open WebUI site.](https://openwebui.com/f/coleam/n8n_pipe/) (more instructions below)

![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif)

Curated by <https://github.com/n8n-io> and <https://github.com/coleam00>, it combines the self-hosted n8n
platform with a curated list of compatible AI products and components to
quickly get started with building self-hosted AI workflows.

### What‚Äôs included

‚úÖ [**Self-hosted n8n**](https://n8n.io/) - Low-code platform with over 400
integrations and advanced AI components

‚úÖ [**Supabase**](https://supabase.com/) - Open source database as a service -
most widely used database for AI agents

‚úÖ [**Ollama**](https://ollama.com/) - Cross-platform LLM platform to install
and run the latest local LLMs

‚úÖ [**Open WebUI**](https://openwebui.com/) - ChatGPT-like interface to
privately interact with your local models and N8N agents

‚úÖ [**Flowise**](https://flowiseai.com/) - No/low code AI agent
builder that pairs very well with n8n

‚úÖ [**Qdrant**](https://qdrant.tech/) - Open source, high performance vector
store with an comprehensive API. Even though you can use Supabase for RAG, this was
kept unlike Postgres since it's faster than Supabase so sometimes is the better option.

‚úÖ [**Neo4j**](https://neo4j.com/) - Knowledge graph engine that powers tools like GraphRAG, LightRAG, and Graphiti

‚úÖ [**SearXNG**](https://searxng.org/) - Open source, free internet metasearch engine which aggregates
results from up to 229 search services. Users are neither tracked nor profiled, hence the fit with the local AI package.

‚úÖ [**Caddy**](https://caddyserver.com/) - Managed HTTPS/TLS for custom domains

‚úÖ [**Langfuse**](https://langfuse.com/) - Open source LLM engineering platform for agent observability

‚úÖ [**ComfyUI**](https://github.com/comfyanonymous/ComfyUI) - Powerful and modular stable diffusion GUI and backend with nodes/graph interface for advanced workflows

‚úÖ [**Infisical**](https://infisical.com/) - Open source secrets management platform with web UI and CLI for securely managing environment variables and sensitive configuration

‚úÖ [**Lambda Server**](04-lambda/README.md) - FastAPI server providing REST APIs and MCP (Model Context Protocol) endpoints with 40+ tools for RAG, knowledge graphs, workflow automation, and more

‚úÖ [**Discord Bot**](03-apps/discord-bot/README.md) - Discord bot for Immich integration with face mapping, automated notifications, and MCP server for Discord management

‚úÖ [**Immich**](https://immich.app/) - Self-hosted photo and video backup solution with face detection, metadata search, and social features

## Prerequisites

Before you begin, make sure you have the following software installed:

- [Python](https://www.python.org/downloads/) - Required to run the setup script
- [Git/GitHub Desktop](https://desktop.github.com/) - For easy repository management
- [Docker/Docker Desktop](https://www.docker.com/products/docker-desktop/) - Required to run all services

## Installation

Clone the repository and navigate to the project directory:

```bash
git clone -b stable https://github.com/coleam00/local-ai-packaged.git
cd local-ai-packaged
```

Before running the services, you need to set up your environment variables. The easiest way is to use the automatic password generator script.

1. Make a copy of `.env_sample` and rename it to `.env` in the root directory of the project (or the script will create a basic template for you)

2. **Generate secure passwords automatically (Recommended):**

   Run the password generator script to automatically generate secure passwords and keys for your `.env` file:

   ```bash
   python setup/generate-env-passwords.py
   ```

   This script will:
   - Generate XKCD-style passphrases for passwords (e.g., `POSTGRES_PASSWORD`, `DASHBOARD_PASSWORD`)
   - Generate secure hex strings for encryption keys (e.g., `N8N_ENCRYPTION_KEY`)
   - Generate base64 strings for auth secrets (e.g., `INFISICAL_AUTH_SECRET`)
   - Generate Supabase JWT tokens (`JWT_SECRET`, `ANON_KEY`, `SERVICE_ROLE_KEY`)
   - Prompt interactively for Docker Hub credentials (optional)
   - Automatically add missing values to your `.env` file
   - Create a backup of your existing `.env` file

   **Note:** You can skip Docker Hub credentials during the script and add them manually later if needed. You'll also need to add any custom hostnames or domain configurations manually.

3. **Manual setup (Optional - only if you prefer not to use the automatic generator):**

   If you prefer to set environment variables manually, here are the required variables. Note that the automatic script generates all of these for you:

   ```bash
   ############
   # N8N Configuration
   ############
   N8N_ENCRYPTION_KEY=
   N8N_USER_MANAGEMENT_JWT_SECRET=

   ############
   # Supabase Secrets
   ############
   POSTGRES_PASSWORD=
   JWT_SECRET=
   ANON_KEY=
   SERVICE_ROLE_KEY=
   DASHBOARD_USERNAME=
   DASHBOARD_PASSWORD=
   POOLER_TENANT_ID=

   ############
   # Supabase Storage (S3 Compatible)
   ############
   # Optional: Override default MinIO credentials for Supabase storage
   # Defaults: SUPABASE_MINIO_ROOT_USER=supa-storage, SUPABASE_MINIO_ROOT_PASSWORD=secret1234
   SUPABASE_MINIO_ROOT_USER=
   SUPABASE_MINIO_ROOT_PASSWORD=

   ############
   # Neo4j Secrets
   ############
   NEO4J_AUTH=

   ############
   # MongoDB Secrets
   ############
   MONGODB_ROOT_USERNAME=admin
   MONGODB_ROOT_PASSWORD=
   MONGODB_DATABASE=admin
   MONGODB_EXPRESS_USERNAME=admin
   MONGODB_EXPRESS_PASSWORD=

   ############
   # Langfuse credentials
   ############

   CLICKHOUSE_PASSWORD=
   MINIO_ROOT_PASSWORD=
   LANGFUSE_SALT=
   NEXTAUTH_SECRET=
   ENCRYPTION_KEY=

   ############
   # Docker Hub Credentials (Required for dhi.io registry)
   ############
   # Docker Hub username (same credentials used for dhi.io)
   DOCKER_HUB_USERNAME=
   # Docker Hub Personal Access Token (PAT) - STRONGLY RECOMMENDED
   # OR Docker Hub password (less secure)
   DOCKER_HUB_PASSWORD=
   # Alternative: You can also use DOCKER_HUB_TOKEN instead of DOCKER_HUB_PASSWORD
   # DOCKER_HUB_TOKEN=

   ############
   # Infisical Configuration (Optional - for secret management)
   ############
   # These are automatically generated by the password script, or generate manually:
   # INFISICAL_ENCRYPTION_KEY= (generate with: openssl rand -hex 16)
   # INFISICAL_AUTH_SECRET= (generate with: openssl rand -base64 32)
   # For local development (port-based)
   INFISICAL_HOSTNAME=:8020
   # For production (domain-based)
   # INFISICAL_HOSTNAME=infisical.yourdomain.com
   INFISICAL_SITE_URL=http://localhost:8020
   ```

> [!IMPORTANT]
> The automatic password generator script (`python setup/generate-env-passwords.py`) will generate secure random values for all secrets including Supabase JWT tokens. If you're setting up manually, make sure to generate secure random values for all secrets. Never use the example values in production.

## Docker Hardened Images (dhi.io) Authentication

This project uses [Docker Hardened Images (DHI)](https://docs.docker.com/dhi/) from the `dhi.io` registry for enhanced security. These images require authentication using your Docker Hub credentials.

### Setting Up Authentication

1. **Create a Docker Hub Account** (if you don't have one):

   - Sign up for free at https://hub.docker.com/signup

2. **Create a Personal Access Token (PAT)** - **Strongly Recommended**:

   - Go to https://hub.docker.com/settings/security
   - Click "New Access Token"
   - Give it a descriptive name (e.g., "dhi.io-automation")
   - Set appropriate permissions (read-only is sufficient for pulling images)
   - Copy the token immediately (you won't be able to see it again)

3. **Add Credentials to `.env`**:
   ```bash
   DOCKER_HUB_USERNAME=your-docker-hub-username
   DOCKER_HUB_PASSWORD=your-personal-access-token
   # OR use DOCKER_HUB_TOKEN instead:
   # DOCKER_HUB_TOKEN=your-personal-access-token
   ```

### Why Use Personal Access Tokens?

- **Enhanced Security**: PATs can be scoped to specific permissions and revoked independently
- **Better for Automation**: Ideal for CI/CD pipelines and automated deployments
- **Audit Trail**: You can investigate token usage if compromised
- **No Account Impact**: Revoking a PAT doesn't affect your main account password

### Automatic Authentication

The `start_services.py` script automatically authenticates to `dhi.io` before pulling images. Authentication happens:

- Automatically on startup (if credentials are in `.env`)
- Only when needed (skips if already authenticated)
- Securely (passwords passed via stdin, not command line)

### Manual Authentication (Optional)

If you prefer to authenticate manually:

```bash
docker login dhi.io
# Enter your Docker Hub username and password/token when prompted
```

### Troubleshooting

If you encounter authentication errors:

1. Verify `DOCKER_HUB_USERNAME` and `DOCKER_HUB_PASSWORD` are set correctly in `.env`
2. Ensure you're using a valid Personal Access Token or password
3. Check that your Docker Hub account is active
4. Try manually running: `docker login dhi.io`
5. For production deployments, consider using external secret management systems

### Security Best Practices

- ‚úÖ Use Personal Access Tokens instead of passwords
- ‚úÖ Rotate credentials regularly
- ‚úÖ Use least-privilege (minimal required permissions for PATs)
- ‚úÖ Never commit credentials to version control (`.env` should be in `.gitignore`)
- ‚úÖ For production, consider using external secret managers (AWS Secrets Manager, HashiCorp Vault, etc.)

4. Set the following environment variables if deploying with a custom domain:

   **For Cloudflare Tunnel (Recommended - No port forwarding needed):**

   ```bash
   ############
   # Cloudflare Tunnel Configuration
   ############
   CLOUDFLARE_TUNNEL_TOKEN=your-tunnel-token-here

   ############
   # Caddy Hostname Configuration (for datacrew.space)
   ############
   N8N_HOSTNAME=n8n.datacrew.space
   WEBUI_HOSTNAME=webui.datacrew.space
   FLOWISE_HOSTNAME=flowise.datacrew.space
   SUPABASE_HOSTNAME=supabase.datacrew.space
   OLLAMA_HOSTNAME=ollama.datacrew.space
   SEARXNG_HOSTNAME=searxng.datacrew.space
   LANGFUSE_HOSTNAME=langfuse.datacrew.space
   NEO4J_HOSTNAME=neo4j.datacrew.space
   COMFYUI_HOSTNAME=comfyui.datacrew.space
   INFISICAL_HOSTNAME=infisical.datacrew.space
   ```

   See [cloudflare-access-setup skill](.cursor/skills/cloudflare-access-setup/SKILL.md) for detailed Cloudflare setup instructions.

## Infisical Secret Management (Optional)

This project includes [Infisical](https://infisical.com/) for managing secrets and environment variables. Infisical provides a web UI and CLI for securely storing and accessing sensitive configuration.

### Quick Start with Infisical

1. **Install CLI tools** (Infisical, Google Cloud, Cloudflared, and pre-commit):

   **Automated installation (Recommended):**
   ```bash
   python setup/install_clis.py
   ```
   This will install:
   - Infisical CLI (for secret management)
   - Google Cloud CLI (gcloud)
   - Cloudflared CLI (for Cloudflare Tunnel)
   - pre-commit (for code quality hooks)

   **Manual installation:**
   - See installation guide: https://infisical.com/docs/cli/overview
   - Verify: `infisical --version`

2. **Generate encryption keys** and add to `.env`:

   **Automatic (Recommended):**
   ```bash
   python setup/generate-env-passwords.py
   ```
   This will automatically generate and add `INFISICAL_ENCRYPTION_KEY` and `INFISICAL_AUTH_SECRET` to your `.env` file.

   **Manual:**
   ```bash
   # Generate ENCRYPTION_KEY (16-byte hex)
   openssl rand -hex 16

   # Generate AUTH_SECRET (32-byte base64)
   openssl rand -base64 32
   ```

3. **Start services** - Infisical will start automatically:

   ```bash
   python start_services.py --profile gpu-nvidia
   ```

4. **Access Infisical UI**:

   - Local: `http://localhost:8020/admin/signup`
   - Create admin account (first user)
   - Create organization and project
   - Migrate secrets from `.env` to Infisical

5. **Configure CLI access**:
   ```bash
   infisical login
   infisical init
   ```

### Using Infisical

- **Web UI**: Manage secrets via browser at `http://localhost:8020`
- **CLI**: Export secrets automatically when starting services
- **Hybrid approach**: Keep non-sensitive config in `.env`, secrets in Infisical
- **Disable Infisical**: Use `--skip-infisical` flag to use `.env` files only

For Infisical configuration details, see [00-infrastructure/AGENTS.md](00-infrastructure/AGENTS.md).

## Supabase Storage (S3 Compatible)

This setup includes **Supabase Storage with S3-compatible backend** using MinIO. Supabase Storage is configured to use a local MinIO instance for S3-compatible object storage, separate from the Langfuse MinIO instance.

### Features

- ‚úÖ **S3-Compatible Storage**: Full S3 API compatibility for easy integration
- ‚úÖ **Local MinIO Instance**: Dedicated MinIO service for Supabase storage
- ‚úÖ **Image Transformation**: Built-in image optimization and transformation via imgproxy
- ‚úÖ **Fine-grained Access Control**: Row-level security and custom policies
- ‚úÖ **Automatic Bucket Creation**: Initial bucket is created automatically

### Configuration

The S3 storage is automatically enabled when you start the services. The configuration uses:

- **MinIO Service**: `supabase-minio` (separate from Langfuse MinIO)
- **Internal Ports**: 9020 (API), 9021 (Console)
- **External Access** (private mode): `localhost:9020` (API), `localhost:9021` (Console)
- **Default Credentials**: `supa-storage` / `secret1234` (configurable via environment variables)

### Customizing MinIO Credentials

To use custom credentials for Supabase MinIO, add these to your `.env` file:

```bash
SUPABASE_MINIO_ROOT_USER=your-custom-username
SUPABASE_MINIO_ROOT_PASSWORD=your-secure-password
```

> [!IMPORTANT]
> Change the default credentials in production! The default values are for development only.

### Accessing MinIO Console

1. **Local Development** (private mode):

   - Console: `http://localhost:9021`
   - API: `http://localhost:9020`

2. **Using MinIO Client (mc)**:

   ```bash
   # Configure alias
   mc alias set supabase-minio http://localhost:9020 supa-storage secret1234

   # List buckets
   mc ls supabase-minio
   ```

### Using Supabase Storage

Supabase Storage is accessible via the Supabase API at your configured Supabase hostname. For more information, see the [Supabase Storage documentation](https://supabase.com/docs/guides/storage).

**For direct DNS (requires port forwarding):**

```bash
############
# Caddy Config
############
N8N_HOSTNAME=n8n.yourdomain.com
WEBUI_HOSTNAME=webui.yourdomain.com
FLOWISE_HOSTNAME=flowise.yourdomain.com
SUPABASE_HOSTNAME=supabase.yourdomain.com
OLLAMA_HOSTNAME=ollama.yourdomain.com
SEARXNG_HOSTNAME=searxng.yourdomain.com
LANGFUSE_HOSTNAME=langfuse.yourdomain.com
NEO4J_HOSTNAME=neo4j.yourdomain.com
COMFYUI_HOSTNAME=comfyui.yourdomain.com
LETSENCRYPT_EMAIL=your-email-address
```

**For local development (port-based access):**

```bash
# Leave hostnames as port-based defaults or set explicitly:
N8N_HOSTNAME=:8001
WEBUI_HOSTNAME=:8002
FLOWISE_HOSTNAME=:8003
# etc.
```

---

The project includes a `start_services.py` script that handles starting both the Supabase and local AI services. The script accepts a `--profile` flag to specify which GPU configuration to use.

### For Nvidia GPU users

```bash
python start_services.py --profile gpu-nvidia
```

> [!NOTE]
> If you have not used your Nvidia GPU with Docker before, please follow the
> [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.mdx).

### For AMD GPU users on Linux

```bash
python start_services.py --profile gpu-amd
```

### For Mac / Apple Silicon users

If you're using a Mac with an M1 or newer processor, you can't expose your GPU to the Docker instance, unfortunately. There are two options in this case:

1. Run the starter kit fully on CPU:

   ```bash
   python start_services.py --profile cpu
   ```

2. Run Ollama on your Mac for faster inference, and connect to that from the n8n instance:

   ```bash
   python start_services.py --profile none
   ```

   If you want to run Ollama on your mac, check the [Ollama homepage](https://ollama.com/) for installation instructions.

#### For Mac users running OLLAMA locally

If you're running OLLAMA locally on your Mac (not in Docker), you need to configure n8n to connect to your local Ollama instance:

1. After you see "Editor is now accessible via: http://localhost:5678/", navigate to the n8n UI
2. Go to **Settings ‚Üí Credentials ‚Üí Add Credential**
3. Select **"Ollama"** from the credential types
4. Set the **Base URL** to: `http://host.docker.internal:11434`
5. Leave **API Key** empty (not needed for local Ollama)
6. Save the credential with a name like "Local Ollama service"

**For Docker users (default setup):**

If Ollama is running in Docker (same as n8n), configure the credential with:
- **Base URL**: `http://ollama:11434` (container name on ai-network)

Both services are on the same Docker network, so they can communicate using container names.

### For everyone else

```bash
python start_services.py --profile cpu
```

### The environment argument

The **start-services.py** script offers the possibility to pass one of two options for the environment argument, **private** (default environment) and **public**:

- **private:** you are deploying the stack in a safe environment, hence a lot of ports can be made accessible without having to worry about security
- **public:** the stack is deployed in a public environment, which means the attack surface should be made as small as possible. All ports except for 80 and 443 are closed

The stack initialized with

```bash
   python start_services.py --profile gpu-nvidia --environment private
```

equals the one initialized with

```bash
   python start_services.py --profile gpu-nvidia
```

## Deploying to the Cloud

### Option 1: Cloudflare Tunnel (Recommended)

**Cloudflare Tunnel is the recommended approach** - it requires no port forwarding, works behind NAT/firewalls, and hides your origin IP. Perfect for home servers or any environment where you can't open ports.

**Prerequisites:**

- Cloudflare account (free tier works)
- Domain added to Cloudflare
- `cloudflared` CLI installed (for automated setup)

**Automated Setup (Recommended):**

Run the automated setup script:

```bash
python setup/cloudflare/setup_tunnel.py
```

This script will:

- Check for cloudflared CLI installation
- Authenticate with Cloudflare (opens browser)
- Create a tunnel
- Configure DNS records for all services
- Generate tunnel token and update `.env` file
- Guide you through configuring public hostnames

**Manual Setup:**

If you prefer manual setup, follow the [cloudflare-access-setup skill](.cursor/skills/cloudflare-access-setup/SKILL.md)

**After Setup:**

1. Verify your `.env` file has `CLOUDFLARE_TUNNEL_TOKEN` and domain hostnames
2. Start services normally - no port forwarding needed!

```bash
python start_services.py --profile gpu-nvidia
```

**Benefits:**

- ‚úÖ No port forwarding required
- ‚úÖ Works with dynamic IP addresses
- ‚úÖ Origin IP completely hidden
- ‚úÖ Free SSL certificates
- ‚úÖ DDoS protection included
- ‚úÖ No firewall/router configuration needed

### Option 2: Direct DNS with Port Forwarding

For cloud instances with static IPs where you can open ports:

**Prerequisites:**

- Linux machine (preferably Ubuntu) with Nano, Git, and Docker installed
- Static public IP address
- Ability to configure firewall/router for port forwarding

**Extra steps:**

1. Run the commands as root to open up the necessary ports:

   - ufw enable
   - ufw allow 80 && ufw allow 443
   - ufw reload

   ***

   **WARNING**

   ufw does not shield ports published by docker, because the iptables rules configured by docker are analyzed before those configured by ufw. There is a solution to change this behavior, but that is out of scope for this project. Just make sure that all traffic runs through the caddy service via port 443. Port 80 should only be used to redirect to port 443.

   ***

2. Run the **start-services.py** script with the environment argument **public** to indicate you are going to run the package in a public environment. The script will make sure that all ports, except for 80 and 443, are closed down, e.g.

```bash
   python3 start_services.py --profile gpu-nvidia --environment public
```

3. Set up A records for your DNS provider to point your subdomains you'll set up in the .env file for Caddy
   to the IP address of your cloud instance.

   For example, A record to point n8n to [cloud instance IP] for n8n.yourdomain.com

**NOTE**: If you are using a cloud machine without the "docker compose" command available by default, such as a Ubuntu GPU instance on DigitalOcean, run these commands before running start_services.py:

- DOCKER_COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep 'tag_name' | cut -d\\" -f4)
- sudo curl -L "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-linux-x86_64" -o /usr/local/bin/docker-compose
- sudo chmod +x /usr/local/bin/docker-compose
- sudo mkdir -p /usr/local/lib/docker/cli-plugins
- sudo ln -s /usr/local/bin/docker-compose /usr/local/lib/docker/cli-plugins/docker-compose

## MCP Server Integration

The Lambda server provides a comprehensive **Model Context Protocol (MCP)** server with 40+ tools organized into categories:

### Available MCP Tool Categories

- **MongoDB RAG Tools**: Search knowledge base, ingest documents, conversational agent, code example search
- **Graphiti RAG Tools**: Knowledge graph search, GitHub repository parsing, hallucination detection
- **Crawl4AI Tools**: Single page crawling, deep website crawling, web search
- **N8N Workflow Tools**: Create, update, delete, activate, and execute workflows
- **Open WebUI Tools**: Export conversations, classify topics, search conversations
- **Calendar Tools**: Create, update, delete, and list Google Calendar events
- **Knowledge Extraction**: Extract events from web content and crawled pages
- **Persona Management**: Store facts, search memories, manage persona state and mood
- **Conversation Orchestration**: Multi-agent conversation management

### Connecting to MCP Server

The Lambda MCP server is accessible at `http://lambda-server:8000/mcp` (internal) or via your configured hostname.

**For Open WebUI:**
1. Go to Settings ‚Üí Connections ‚Üí MCP Servers
2. Add new server: `http://lambda-server:8000/mcp`
3. Tools will be automatically available in conversations

**For Claude Desktop:**
Add to your MCP settings:
```json
{
  "mcpServers": {
    "lambda-server": {
      "url": "http://localhost:8000/mcp"
    }
  }
}
```

For MCP troubleshooting and connection guide, see the [mcp-troubleshooting skill](.cursor/skills/mcp-troubleshooting/SKILL.md). For tool catalog, see [04-lambda/AGENTS.md](04-lambda/AGENTS.md).

## Workflow Overview

The local-ai-packaged infrastructure supports several key workflows:

### Document Ingestion Workflow
1. **Upload/Crawl**: Documents uploaded via REST API or crawled from web pages
2. **Processing**: Documents are chunked, embedded, and stored in MongoDB
3. **Indexing**: Vector embeddings enable semantic search, full-text search enables keyword matching
4. **Search**: Hybrid search combines semantic and text search using Reciprocal Rank Fusion

### RAG Search Workflow
1. **Query**: User submits natural language query
2. **Search**: Lambda server searches MongoDB RAG knowledge base (semantic + text)
3. **Retrieval**: Relevant chunks retrieved with metadata
4. **Generation**: LLM (Ollama) generates response using retrieved context
5. **Response**: Formatted answer returned to user

### Conversation Memory Workflow
1. **Chat**: User interacts with Open WebUI
2. **Storage**: Conversations stored in PostgreSQL
3. **Export**: Conversations can be exported to MongoDB RAG for searchability
4. **Topic Classification**: LLM classifies conversations into topics
5. **Search**: Users can search past conversations by topic or content

### Discord ‚Üí Immich Workflow
1. **Upload**: User uploads photo/video to Discord `#event-uploads` channel
2. **Ingestion**: Discord bot automatically uploads to Immich
3. **Face Detection**: Immich detects faces in uploaded media
4. **Mapping**: Users claim faces via `/claim_face` command
5. **Notifications**: Bot sends DMs when users are detected in new photos

### N8N Workflow Orchestration
1. **Trigger**: External event (webhook, schedule, file change, etc.)
2. **Processing**: N8N workflow executes with Lambda MCP tools
3. **Integration**: Lambda tools interact with RAG, knowledge graphs, calendars, etc.
4. **Response**: Results returned via webhook or stored in database

For detailed workflow documentation, see the capability AGENTS.md files in [04-lambda/src/](04-lambda/src/).

## ‚ö°Ô∏è Quick start and usage

The main component of the self-hosted AI starter kit uses a **modular Docker Compose architecture** where each major component has its own `docker-compose.yml` file in numbered stack directories (`00-infrastructure/`, `01-data/`, etc.). This approach eliminates conflicts, makes updates easier, and provides better organization. See [AGENTS.md](AGENTS.md) for architecture details.

After completing the installation steps above, follow the steps below to get started.

1. Open <http://localhost:5678/> in your browser to set up n8n. You‚Äôll only
   have to do this once. You are NOT creating an account with n8n in the setup here,
   it is only a local account for your instance!
2. Open the included workflow:
   <http://localhost:5678/workflow/vTN9y2dLXqTiDfPT>
3. Create credentials for every service:

   Ollama URL: http://ollama:11434

   Postgres (through Supabase): use DB, username, and password from .env. IMPORTANT: Host is 'db'
   Since that is the name of the service running Supabase

   Qdrant URL: http://qdrant:6333 (API key can be whatever since this is running locally)

   Google Drive: Follow [this guide from n8n](https://docs.n8n.io/integrations/builtin/credentials/google/).
   Don't use localhost for the redirect URI, just use another domain you have, it will still work!
   Alternatively, you can set up [local file triggers](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/).

4. Select **Test workflow** to start running the workflow.
5. If this is the first time you‚Äôre running the workflow, you may need to wait
   until Ollama finishes downloading Llama3.1. You can inspect the docker
   console logs to check on the progress.
6. Make sure to toggle the workflow as active and copy the "Production" webhook URL!
7. Open <http://localhost:3000/> in your browser to set up Open WebUI.
   - **Google OAuth**: If `ENABLE_OAUTH_SIGNUP=true` is set in your `.env`, you can sign in with Google
   - **Local Account**: You can also create a local account (if `ENABLE_LOGIN_FORM=true`)
   - You'll only have to do this once. Accounts are stored in PostgreSQL for persistence.
8. Go to Workspace -> Functions -> Add Function -> Give name + description then paste in
   the code from `n8n_pipe.py`

   The function is also [published here on Open WebUI's site](https://openwebui.com/f/coleam/n8n_pipe/).

9. Click on the gear icon and set the n8n_url to the production URL for the webhook
   you copied in a previous step.
10. Toggle the function on and now it will be available in your model dropdown in the top left!

To open n8n at any time, visit <http://localhost:5678/> in your browser.
To open Open WebUI at any time, visit <http://localhost:3000/>.

With your n8n instance, you‚Äôll have access to over 400 integrations and a
suite of basic and advanced AI nodes such as
[AI Agent](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/),
[Text classifier](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/),
and [Information Extractor](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/)
nodes. To keep everything local, just remember to use the Ollama node for your
language model and Qdrant as your vector store.

> [!NOTE]
> This starter kit is designed to help you get started with self-hosted AI
> workflows. While it‚Äôs not fully optimized for production environments, it
> combines robust components that work well together for proof-of-concept
> projects. You can customize it to meet your specific needs

## Upgrading

To update all containers to their latest versions (n8n, Open WebUI, etc.), run these commands:

```bash
# Stop all services
python start_services.py --action stop

# Start services again (this will automatically pull the latest images)
python start_services.py --profile <your-profile>
```

Replace `<your-profile>` with one of: `cpu`, `gpu-nvidia`, `gpu-amd`, or `none`.

Note: The `start_services.py` script automatically pulls the latest images for your configured services before starting them.

## Troubleshooting

Here are solutions to common issues you might encounter:

### Supabase Issues

- **Supabase Pooler Restarting**: If the supabase-pooler container keeps restarting itself, follow the instructions in [this GitHub issue](https://github.com/supabase/supabase/issues/30210#issuecomment-2456955578).

- **Supabase Analytics Startup Failure**: If the supabase-analytics container fails to start after changing your Postgres password, delete the folder `supabase/docker/volumes/db/data`.

- **If using Docker Desktop**: Go into the Docker settings and make sure "Expose daemon on tcp://localhost:2375 without TLS" is turned on

- **Service Conflicts** - If you see "conflicts with imported resource" errors, ensure you're using the stack-based compose files in numbered directories (`00-infrastructure/`, `01-data/`, etc.). Use `python start_services.py` to manage stacks.

- **Container Name Conflicts** - If you get "container name already in use" errors, the startup script should automatically clean these up. If issues persist, manually remove containers: `docker rm -f <container-name>`

- **Network Issues** - If services can't communicate, verify the `localai_default` network exists: `docker network inspect localai_default`. The network is created by `compose/core/docker-compose.yml`.

- **Supabase Service Unavailable** - Make sure you don't have an "@" character in your Postgres password! If the connection to the kong container is working (the container logs say it is receiving requests from n8n) but n8n says it cannot connect, this is generally the problem from what the community has shared. Other characters might not be allowed too, the @ symbol is just the one I know for sure!

- **SearXNG Restarting**: If the SearXNG container keeps restarting, run the command "chmod 755 searxng" within the local-ai-packaged folder so SearXNG has the permissions it needs to create the uwsgi.ini file.

- **Files not Found in Supabase Folder** - If you get any errors around files missing in the supabase/ folder like .env, docker/docker-compose.yml, etc. this most likely means you had a "bad" pull of the Supabase GitHub repository when you ran the start_services.py script. Delete the supabase/ folder within the Local AI Package folder entirely and try again.

### GPU Support Issues

- **Windows GPU Support**: If you're having trouble running Ollama with GPU support on Windows with Docker Desktop:

  1. Open Docker Desktop settings
  2. Enable WSL 2 backend
  3. See the [Docker GPU documentation](https://docs.docker.com/desktop/features/gpu/) for more details

- **Linux GPU Support**: If you're having trouble running Ollama with GPU support on Linux, follow the [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.md).

## üëì Recommended reading

n8n is full of useful content for getting started quickly with its AI concepts
and nodes. If you run into an issue, go to [support](#support).

- [AI agents for developers: from theory to practice with n8n](https://blog.n8n.io/ai-agents/)
- [Tutorial: Build an AI workflow in n8n](https://docs.n8n.io/advanced-ai/intro-tutorial/)
- [Langchain Concepts in n8n](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/)
- [Demonstration of key differences between agents and chains](https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/)
- [What are vector databases?](https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/)

## üé• Video walkthrough

- [Cole's Guide to the Local AI Starter Kit](https://youtu.be/pOsO40HSbOo)

## üõçÔ∏è More AI templates

For more AI workflow ideas, visit the [**official n8n AI template
gallery**](https://n8n.io/workflows/?categories=AI). From each workflow,
select the **Use workflow** button to automatically import the workflow into
your local n8n instance.

### Learn AI key concepts

- [AI Agent Chat](https://n8n.io/workflows/1954-ai-agent-chat/)
- [AI chat with any data source (using the n8n workflow too)](https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/)
- [Chat with OpenAI Assistant (by adding a memory)](https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/)
- [Use an open-source LLM (via HuggingFace)](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)
- [Chat with PDF docs using AI (quoting sources)](https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/)
- [AI agent that can scrape webpages](https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/)

### Local AI templates

- [Tax Code Assistant](https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/)
- [Breakdown Documents into Study Notes with MistralAI and Qdrant](https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/)
- [Financial Documents Assistant using Qdrant and](https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/)¬†[¬†Mistral.ai](http://mistral.ai/)
- [Recipe Recommendations with Qdrant and Mistral](https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/)

## Development

For information about development workflow, code quality tools, pre-commit hooks, and CI/CD, see [docs/DEVELOPMENT.md](docs/DEVELOPMENT.md).

Quick start for developers:

```bash
# Install CLI tools including pre-commit (recommended)
python setup/install_clis.py

# Or install pre-commit manually
pip install pre-commit
pre-commit install

# Run code quality checks
pre-commit run --all-files

# Format code
black --line-length=100 .
ruff format .

# Run tests
cd 04-lambda && uv run pytest tests/ -v
```

## Tips & tricks

### Accessing local files

The self-hosted AI starter kit will create a shared folder (by default,
located in the same directory) which is mounted to the n8n container and
allows n8n to access files on disk. This folder within the n8n container is
located at `/data/shared` -- this is the path you‚Äôll need to use in nodes that
interact with the local filesystem.

**Nodes that interact with the local filesystem**

- [Read/Write Files from Disk](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/)
- [Local File Trigger](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/)
- [Execute Command](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/)

## üìú¬†License

This project (originally created by the n8n team, link at the top of the README) is licensed under the Apache License 2.0 - see the
[LICENSE](LICENSE) file for details.
